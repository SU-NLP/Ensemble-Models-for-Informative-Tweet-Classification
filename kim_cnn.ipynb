{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "kim_cnn.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ea6ssaxPFi0D"
      },
      "source": [
        "# data source\n",
        "! git clone https://github.com/VinAIResearch/COVID19Tweet.git\n",
        "\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.models import Model\n",
        "from keras.callbacks import EarlyStopping,ModelCheckpoint,ReduceLROnPlateau\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QI_MV9WLFcoY"
      },
      "source": [
        "# loading imports from args file\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slhElqMSFbOR"
      },
      "source": [
        "args_file = 'cnn_args.json'\n",
        "\n",
        "with open(args_file) as f:\n",
        "  args = json.load(f)\n",
        "\n",
        "\n",
        "# data_dir = args['data_dir'] # files must be train.tsv and valid.tsv \n",
        "num_words = args['num_words'] # how many words to keep in vocab\n",
        "max_seq_len = args['max_seq_len'] # max length of a seq vector (for padding or cropping)\n",
        "# pretrained_embeds_file = args['pretrained_embeds_file']\n",
        "filter_widths = args['filter_widths']\n",
        "number_of_filters = args['number_of_filters']\n",
        "dropout_prob = args['dropout_prob']\n",
        "optimizer = args['optimizer']\n",
        "n_classes = args['n_classes']\n",
        "hidden_activation = args['hidden_activation']\n",
        "BATCH_SIZE = args['BATCH_SIZE']\n",
        "max_epochs = args['max_epochs'] \n",
        "trained_model_dir = args['trained_model_dir']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxREIJD8Fltf"
      },
      "source": [
        "# loading task data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wz_CXhd8FpFr"
      },
      "source": [
        "train_df = pd.read_csv(\"COVID19Tweet/train.tsv\", sep='\\t')\n",
        "val_df = pd.read_csv(\"COVID19Tweet/valid.tsv\", sep='\\t',names=['Id','Text','Label'])\n",
        "test_df = pd.read_csv(\"COVID19Tweet/unlabeled_test_with_noise.tsv\", sep='\\t',names=['Id','Text'])\n",
        "\n",
        "\n",
        "train_sentences = train_df.Text.values\n",
        "train_labels =  train_df.Label.values\n",
        "\n",
        "val_sentences = val_df.Text.values\n",
        "val_labels =  val_df.Label.values\n",
        "\n",
        "\n",
        "test_sentences = test_df.Text.values\n",
        "# test_labels =  val_df.Label.values\n",
        "\n",
        "y_train = [int(label == 'INFORMATIVE') for label in train_labels]\n",
        "y_val = [int(label == 'INFORMATIVE') for label in val_labels]\n",
        "\n",
        "y_train = np.array(y_train)\n",
        "y_val = np.array(y_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4G6BuYJFrti"
      },
      "source": [
        "# preparing text input for network using Keras tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qyl4D_WFz1H"
      },
      "source": [
        "tokenizer = Tokenizer(num_words=num_words)\n",
        "tokenizer.fit_on_texts(train_sentences)\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences(train_sentences)\n",
        "X_val = tokenizer.texts_to_sequences(val_sentences)\n",
        "X_test = tokenizer.texts_to_sequences(test_sentences)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n",
        "\n",
        "\n",
        "\n",
        "X_train = pad_sequences(X_train, padding='post', maxlen=max_seq_len)\n",
        "X_val = pad_sequences(X_val, padding='post', maxlen=max_seq_len)\n",
        "X_test = pad_sequences(X_test, padding='post', maxlen=max_seq_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wt_MWywsGaFa"
      },
      "source": [
        "# loading pre-trained embeds as weights matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIX4xAziHFs_"
      },
      "source": [
        "# Either one of these must be chosen to load the embeddings weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdR55-nufk73"
      },
      "source": [
        "## Glove Twitter Embeds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ALSuAJNrbwt",
        "outputId": "93094aed-2bd0-4b3e-f714-973f411ec522",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "! wget http://nlp.stanford.edu/data/wordvecs/glove.twitter.27B.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-08-09 09:40:23--  http://nlp.stanford.edu/data/wordvecs/glove.twitter.27B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/wordvecs/glove.twitter.27B.zip [following]\n",
            "--2020-08-09 09:40:23--  https://nlp.stanford.edu/data/wordvecs/glove.twitter.27B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/wordvecs/glove.twitter.27B.zip [following]\n",
            "--2020-08-09 09:40:23--  http://downloads.cs.stanford.edu/nlp/data/wordvecs/glove.twitter.27B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1520408741 (1.4G) [application/zip]\n",
            "Saving to: ‘glove.twitter.27B.zip’\n",
            "\n",
            "glove.twitter.27B.z 100%[===================>]   1.42G  2.07MB/s    in 11m 41s \n",
            "\n",
            "2020-08-09 09:52:04 (2.07 MB/s) - ‘glove.twitter.27B.zip’ saved [1520408741/1520408741]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_f6aXDpreHM",
        "outputId": "96b06cd3-74e9-4958-e239-55e7833416d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "!unzip glove.twitter.27B.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  glove.twitter.27B.zip\n",
            "  inflating: glove.twitter.27B.100d.txt  \n",
            "  inflating: glove.twitter.27B.200d.txt  \n",
            "  inflating: glove.twitter.27B.25d.txt  \n",
            "  inflating: glove.twitter.27B.50d.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQDBf3oK91Qj",
        "outputId": "be5f51bf-4e3f-4955-c626-fe2a5d0fe85d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from tqdm import tqdm\n",
        "pretrained_embeds_file = 'glove.twitter.27B.200d.txt'\n",
        "\n",
        "\n",
        "embedding_vector = {}\n",
        "f = open(pretrained_embeds_file)\n",
        "for line in tqdm(f):\n",
        "    value = line.split(' ')\n",
        "    word = value[0]\n",
        "    coef = np.array(value[1:],dtype = 'float32')\n",
        "    embedding_vector[word] = coef"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1193517it [01:02, 19007.90it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PkDgquerhr7",
        "outputId": "3da0eb83-e43d-4914-febc-c6154b0fe3cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "EMBEDDING_DIM = embedding_vector['test'].shape[0]\n",
        "\n",
        "embedding_matrix = np.zeros((vocab_size,EMBEDDING_DIM))\n",
        "for word,i in tqdm(tokenizer.word_index.items()):\n",
        "    embedding_value = embedding_vector.get(word)\n",
        "    if embedding_value is not None:\n",
        "        embedding_matrix[i] = embedding_value"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 22935/22935 [00:00<00:00, 537644.13it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WzczvG7fqE8"
      },
      "source": [
        "## W2V Twitter Embeds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycXguUgXfvJg",
        "outputId": "8fca5108-21e4-45c7-9885-7f36fcde63c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "\n",
        "w2v_model = KeyedVectors.load_word2vec_format('drive/My Drive/W-NUT COVID19/word2vec_twitter_tokens.bin', binary=True, unicode_errors='ignore')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:254: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnHzkk4U1oTd",
        "outputId": "efb91e36-e8db-43ae-a3c3-73026d2c316b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "w2v_model['test'].shape[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "400"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10Cd8cb-gFmC",
        "outputId": "5da033bf-82e4-4076-e5b5-19ed6818f62f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from tqdm import tqdm\n",
        "EMBEDDING_DIM = w2v_model['test'].shape[0]\n",
        "\n",
        "embedding_matrix = np.zeros((vocab_size,EMBEDDING_DIM))\n",
        "for word,i in tqdm(tokenizer.word_index.items()):\n",
        "  if word in w2v_model:\n",
        "    embedding_matrix[i] = w2v_model[word]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 22935/22935 [00:00<00:00, 203461.43it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvQ4yUWyGyl5"
      },
      "source": [
        "# kim's CNN network implementation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXc88QMuHQCB"
      },
      "source": [
        "input_text = layers.Input(shape=(max_seq_len,))\n",
        "\n",
        "embedding_layer = layers.Embedding(vocab_size, EMBEDDING_DIM, weights=[embedding_matrix],\n",
        "                            trainable=True)(input_text)\n",
        "text_embed = layers.SpatialDropout1D(dropout_prob)(embedding_layer)\n",
        "\n",
        "\n",
        "conv_layers = []\n",
        "for filter_length in filter_widths:\n",
        "    conv_layer = layers.Conv1D(filters=number_of_filters, kernel_size=filter_length, padding='valid',\n",
        "                        strides=1, activation=hidden_activation)(text_embed)\n",
        "    maxpooling = layers.MaxPool1D(pool_size=max_seq_len - filter_length + 1)(conv_layer)\n",
        "    flatten = layers.Flatten()(maxpooling)\n",
        "    conv_layers.append(flatten)\n",
        "sentence_embed = layers.concatenate(inputs=conv_layers)\n",
        "dropout = layers.Dropout(dropout_prob)(sentence_embed)\n",
        "# dense_layer = layers.Dense(hidden_units, activation=hidden_activation)(dropout)\n",
        "\n",
        "if(n_classes == 2):\n",
        "  output = layers.Dense(1, activation='sigmoid')(dropout)\n",
        "  model = Model(input_text, output)\n",
        "  model.compile(loss='binary_crossentropy', metrics=['acc'], optimizer=optimizer)\n",
        "else:\n",
        "  output = layers.Dense(n_classes, activation='softmax')(dropout)\n",
        "  model = Model(input_text, output)\n",
        "  model.compile(loss='categorical_crossentropy', metrics=['acc'], optimizer=optimizer)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEC_QGQnHXxx"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIcGaD93FBmI"
      },
      "source": [
        "earlyStopping = EarlyStopping(monitor='val_loss', patience=3, verbose=0, mode='min')\n",
        "mcp_save = ModelCheckpoint(trained_model_dir+\"model.hdf5\", save_best_only=True, monitor='val_loss', mode='min')\n",
        "reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, verbose=1, epsilon=1e-4, mode='min')\n",
        "\n",
        "if not os.path.exists(trained_model_dir):\n",
        "    os.makedirs(trained_model_dir)\n",
        "\n",
        "\n",
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=max_epochs,\n",
        "                    verbose=True,\n",
        "                    validation_data=(X_val, y_val), \n",
        "                    callbacks=[earlyStopping, mcp_save, reduce_lr_loss],\n",
        "                    batch_size=BATCH_SIZE)\n",
        "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "loss, accuracy = model.evaluate(X_val, y_val, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvAH4GFfHmP_"
      },
      "source": [
        "# eval model on val set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlOunvilHtBj"
      },
      "source": [
        "cnn_probs = model.predict(X_val)\n",
        "cnn_preds = [int(pred[0]>0.5)for pred in cnn_preds]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SW9ds9YYHoKH"
      },
      "source": [
        "## we only look at class 1 f1-score\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(y_val, cnn_preds,digits=6))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaeAToZDHiBI"
      },
      "source": [
        "# saving CNN prob outputs in CSV to use later in Ensemble "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFYGdEwTH14m"
      },
      "source": [
        "val_results = pd.DataFrame()\n",
        "\n",
        "# depends on which embeds were used\n",
        "val_results['cnn_w2v_outputs'] = cnn_probs.flatten()\n",
        "# val_results['cnn_glove_outputs'] = cnn_probs.flatten()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwYPTV27JX5B"
      },
      "source": [
        "val_results.to_csv(\"val_probs.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}